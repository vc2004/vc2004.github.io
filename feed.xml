<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Thinking and Learning in Networking</title>
    <link href="http://vc2004.github.io/feed.xml" rel="self" />
    <link href="http://vc2004.github.io/" />
    <updated>2016-09-22T12:03:42+08:00</updated>
    <id>http://vc2004.github.io/</id>
    <entry>
        <title type="html"><![CDATA[Linux Networking Optimisation Guide Part I]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/linux-networking-optimisation-guide.html"/>
        <published>2016-09-21T00:00:00+08:00</published>
        <updated>2016-09-21T11:05:01+08:00</updated>
        <id>http://vc2004.github.io/2016/linux-networking-optimisation-guide.html</id>
        <category scheme="http://vc2004.github.io/tag/networking/" term="networking" label="networking" />
        <category scheme="http://vc2004.github.io/tag/linux/" term="Linux" label="Linux" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <h3 id="toc_0">Foreword:</h3>

<ul>
<li>Plain Linux installation is NOT optimised for the best networking performance</li>
<li>Almost all the optimisations have side affect. It is better to test before using it.</li>
</ul>

<h3 id="toc_1">Interrupt Affinity</h3>
<p>CPU Affinity is the most important and most effective optimisation, also it is the entry level optimisation.</p>
<p>Turn off irqbalance if any, note it may cause performance issue on other Hardware/IO devices.</p>

<pre><code>/etc/init.d/irqbalance stop</code></pre>
<p>The Rx queue could be checked by -l and modified by -L</p>

<pre><code># ethtool -l eth4
Channel parameters for eth4:
Pre-set maximums:
RX:        0
TX:        0
Other:        1
Combined:    63
Current hardware settings:
RX:        0
TX:        0
Other:        1
Combined:    32</code></pre>
<p>Check the interrupt number related to the eth0</p>

<pre><code># egrep &quot;CPU0|eth0&quot; /proc/interrupts
            CPU0       CPU1       CPU2       CPU3       CPU4       CPU5       CPU6       CPU7       CPU8       CPU9       CPU10      CPU11      CPU12      CPU13      CPU14      CPU15      CPU16      CPU17      CPU18      CPU19      CPU20      CPU21      CPU22      CPU23      CPU24      CPU25      CPU26      CPU27      CPU28      CPU29      CPU30      CPU31
 148:     347358          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0
 150:         18    1152920          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-0
 151:         27          0      61465          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-1
 152:         10          0          0      32140          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-2
 153:         37          0          0          0     113157          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-3
 154:         10          0          0          0          0      89395          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-4
 155:         11          0          0          0          0          0      75379          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-5
 156:          8          0          0          0          0          0          0     123974          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-6
 157:          5          0          0          0          0          0          0          0     277624          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-7</code></pre>
<p>Echo the cpu bit mask to related interrupt number</p>

<pre><code>echo 00000001 &gt; /proc/irq/148/smp_affinity</code></pre>
<p>Tips: MAC native calculator is very good at calculating cpu bit mask.</p>

<h3 id="toc_2">Interrupt Coalescence</h3>
<p>Interrupt Coalescence (IC) is the number of usec waited or frames gathered to issue a hardware interrupt. A small value or big value both has side affects. If latency is preferred over throughput, eg. the realtime streaming traffic, a small value or be disabled would be benefit. Otherwise for large throughput, a larger value should be selected.</p>

<pre><code>ethtool -c eth0
Coalesce parameters for eth0:
Adaptive RX: off  TX: off
stats-block-usecs: 999936
sample-interval: 0
pkt-rate-low: 0
pkt-rate-high: 0

rx-usecs: 18
rx-frames: 12
rx-usecs-irq: 18
rx-frames-irq: 2

tx-usecs: 80
tx-frames: 20
tx-usecs-irq: 18
tx-frames-irq: 2

rx-usecs-low: 0
rx-frame-low: 0
tx-usecs-low: 0
tx-frame-low: 0

rx-usecs-high: 0
rx-frame-high: 0
tx-usecs-high: 0
tx-frame-high: 0</code></pre>
<p>Some cards support adaptive changing the parameters, just turn on the adapter rx and tx.
<code>
ethtool -C eth0 adaptive-rx on adaptive-tx on
</code></p>

<h3 id="toc_3">NUMA</h3>
<p>The network performance might increase if the NUMA node is close to the PCIe slot with ethernet card attached. But it is very tricky the performance might drop with the Tx/Rx application is on the different NUMA node or on the same logical core. So do tweaking a lots to get the best performance.</p>
<p>It is known that:</p>

<ul>
<li>Two child process on different numa node with cause L3 miss, so the performance will drop.</li>
<li>Two child process on the same logical core(HT) will cause performance drop.</li>
<li>Performance will be optimised if two child process on same numa node with memory also on that node.</li>
</ul>
<p>For example, if two child process on same core 0 [0,16], the performance will drop. The same will happen on cpu0 and cpu8.</p>
<p>So it is important to decide which cpu to pin Tx/Rx netsurf application, otherwise you might get a drop</p>

<pre><code># python cpu_layout.py --status
============================================================
Core and Socket Information (as reported by '/proc/cpuinfo')
============================================================

cores =  [0, 1, 2, 3, 4, 5, 6, 7]
sockets =  [0, 1]

       Socket 0        Socket 1
       --------        --------
Core 0 [0, 16]         [8, 24]

Core 1 [1, 17]         [9, 25]

Core 2 [2, 18]         [10, 26]

Core 3 [3, 19]         [11, 27]

Core 4 [4, 20]         [12, 28]

Core 5 [5, 21]         [13, 29]

Core 6 [6, 22]         [14, 30]

Core 7 [7, 23]         [15, 31]</code></pre>
<p>For best performance on NUMA, check which NUMA node the PCIe are connected to</p>

<pre><code># lspci -tv
 \-[0000:00]-+-00.0  Intel Corporation Haswell-E DMI2
             +-01.0-[02]--
             +-01.1-[05]--
             +-02.0-[06]--+-00.0  Broadcom Corporation BCM57840 NetXtreme II 10/20-Gigabit Ethernet
             |            \-00.1  Broadcom Corporation BCM57840 NetXtreme II 10/20-Gigabit Ethernet

# cat /sys/devices/pci0000\:00/0000\:00\:02.0/numa_node
0

# cat /sys/devices/pci0000\:00/0000\:00\:02.0/local_cpus
00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00ff00ff</code></pre>
<p>0xFF00FF is CPU0-7 &amp; 16-23, so it is better to set the affinity on CPU0-7 or CPU16-23.</p>
<p>Also, if two or more ports from different NIC are used, make sure they are connected to the same CPU socket.</p>

<h3 id="toc_4">CPU Frequency</h3>
<p>To maximise the CPU frequency to handle the network loads, it could be set through OS.</p>

<pre><code>cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
powersave

echo performance &gt; /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor</code></pre>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[测试一下中文，顺便推荐几门上过的 Data Science 公开课]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/chinese-testing.html"/>
        <published>2016-09-01T00:00:00+08:00</published>
        <updated>2016-09-22T12:03:42+08:00</updated>
        <id>http://vc2004.github.io/2016/chinese-testing.html</id>
        <category scheme="http://vc2004.github.io/tag/r/" term="R" label="R" />
        <category scheme="http://vc2004.github.io/tag/statistic/" term="Statistic" label="Statistic" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <h3 id="toc_0">测试一下中文，顺便推荐几门上过的 Data Science 公开课</h3>
<p>This post is testing if language other than english is working on this site. Also some cool free open course for data science are recommended here.</p>

<h4 id="toc_1">Statistical Learning by Stanford</h4>
<p><a href="https://statlearning.class.stanford.edu">statlearning.class.stanford.edu</a></p>
<p>两个教授 Trevor Hastie 和 Rob Tibshirani 都是大牛, Prof Rob在 Lasso 和 Bootstrap Sampling 都有挺重要的贡献。</p>

<h4 id="toc_2">Machine Learning on Coursera</h4>
<p><a href="https://www.coursera.org/learn/machine-learning">www.coursera.org/learn/machine-learning</a></p>
<p>这个就不用多介绍了。。Ng 的牛课，Coursera 代表课之一，深入浅出。。我还想学一遍。。</p>

<h4 id="toc_3">Data Science Specialisation on Coursera</h4>
<p><a href="https://www.coursera.org/specializations/jhu-data-science">www.coursera.org/specializations/jhu-data-science</a></p>
<p>JHU 开的课，这个课总体来说还可以，但是有点小问题是有些课过于简单，有些课又过于复杂，不如前面两门。</p>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[Networking Fundamental and Recent Advance Reading List]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/recent-networking-advance-readlist.html"/>
        <published>2016-08-03T00:00:00+08:00</published>
        <updated>2016-09-21T14:26:07+08:00</updated>
        <id>http://vc2004.github.io/2016/recent-networking-advance-readlist.html</id>
        <category scheme="http://vc2004.github.io/tag/networking/" term="networking" label="networking" />
        <category scheme="http://vc2004.github.io/tag/sdn/" term="sdn" label="sdn" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <p>Complied by Liang Dong</p>

<h3 id="toc_0">Update 2016-09-20</h3>
<p>2016-09-20 Add two thesis about load balancing service in Cloud Networking</p>

<h3 id="toc_1">Foreword</h3>
<p>This is a small reading list about recent advance on networking. It also cover some basic fundamental knowledges of TCP/IP.</p>

<h3 id="toc_2">Data Center Networking:</h3>
<p><strong>A Scalable, Commodity Data Center Network Architecture</strong>, Mohammad Al-Fares, Alexander Loukissas, Amin Vahdat, University of California, San Diego, Sigcomm 2008</p>
<p><strong>Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network</strong>, Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, Anand Kanagala, Jeff Provost, Jason Simmons, Eiichi Tanda, Jim Wanderer, Urs Hölzle, Stephen Stuart, and Amin Vahdat. Sigcomm 2016.</p>
<p><strong>Inside the Social Network’s (Datacenter) Network, Arjun Roy</strong>, Hongyi Zeng, Jasmeet Bagga, George Porter, and Alex C. Snoeren, Sigcomm 2015</p>
<p><strong>Introducing data center fabric, the next-generation Facebook data centre network</strong>, Alexey Andreyev, Facebook</p>

<h3 id="toc_3">Software Defined Networking:</h3>
<p><strong>Openflow: Enable Innovation in Campus Networks</strong>, Nick McKeown, Tom Anderson, Hari Balakrishnan, Guru Parulkar, Larry Peterson, Jennifer Rexford, Scott Shenker, Jonathan Turner, CCR 2008</p>
<p><strong>The Future of Networking, and the Past of Protocols</strong> - Scott Shenker</p>
<p><strong>The Design and Implementation of Open vSwitch</strong>, Ben Pfaff, Justin Pettit, Teemu Koponen, Ethan Jackson, Andy Zhou, Jarno Rajahalme, Jesse Gross, Alex Wang, Joe Stringer, and Pravin Shelar, VMware, Inc.; Keith Amidon, Awake Networks; Martín Casado, VMware, Inc, NSDI 2015</p>
<p><strong>B4: Experience with a Globally-Deployed Software Defined WAN</strong>, Sushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong, Leon Poutievski, Arjun Singh, Subbaiah Venkata, Jim Wanderer, Junlan Zhou, Min Zhu, Jonathan Zolla, Urs Hölzle, Stephen Stuart and Amin Vahdat, Sigcomm 2013</p>

<h3 id="toc_4">Network Virtualisation &amp; Cloud Networking:</h3>
<p><strong>VL2: A Scalable and Flexible Data Centre Network</strong>, Albert Greenberg, Srikanth Kandula,  David A. Maltz , James R. Hamilton, Changhoon Kim, Parveen Patel , Navendu Jain, Parantap Lahiri, Sudipta Sengupta, Microsoft Research, Sigcomm 2009</p>
<p><strong>Network Virtualization in Multi-tenant Datacenters</strong>, Teemu Koponen, Keith Amidon, Peter Balland, Martín Casado, Anupam Chanda, Bryan Fulton, Igor Ganichev, Jesse Gross, Natasha Gude, Paul Ingram, Ethan Jackson, Andrew Lambeth, Romain Lenglet, Shih-Hao Li, Amar Padmanabhan, Justin Pettit, Ben Pfaff, and Rajiv Ramanathan, VMware; Scott Shenker, International Computer Science Institute and the University of California, Berkeley; Alan Shieh, Jeremy Stribling, Pankaj Thakkar, Dan Wendlandt, Alexander Yip, and Ronghua Zhang, VMware, NSDI 2014</p>
<p><strong>Maglev: A Fast and Reliable Software Network Load Balancer</strong>, Daniel E. Eisenbud, Cheng Yi, Carlo Contavalli, Cody Smith, Roman Kononov, Eric Mann-Hielscher, Ardas Cilingiroglu, and Bin Cheyney, Google Inc.; Wentao Shang, University of California, Los Angeles; Jinnah Dylan Hosein, SpaceX</p>
<p><strong>Ananta: Cloud Scale Load Balancing</strong>, Parveen Patel, Deepak Bansal, Lihua Yuan, Ashwin Murthy, Albert Greenberg, David A. Maltz, Randy Kern, Hemant Kumar, Marios Zikos, Hongyu Wu, Changhoon Kim, Naveen Karri, Microsoft.</p>

<h3 id="toc_5">Transport Protocols:</h3>
<p><strong>RFC 5681, TCP Congestion Control</strong>, IETF</p>
<p><strong>Data Center TCP (DCTCP)</strong>, Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, Murari Sridharan, Sigcomm 2010</p>
<p><strong>Understanding TCP Incast Throughput Collapse in Datacenter Networks</strong>, Yanpei Chen, Rean Griffith, Junda Liu, Randy H. Katz, Anthony D. Joseph, Sigcomm 2009</p>

<h3 id="toc_6">Network Measurement:</h3>
<p><strong>Pingmesh: A Large-Scale System for Data CenterNetwork Latency Measurement and Analysis</strong>, Chuanxiong Guo, Lihua Yuan, Dong Xiang, Yingnong Dang, Ray Huang, Dave Maltz, Zhaoyi Liu, Vin Wang, Bin Pang, Hua Chen, Zhi-Wei Lin, Varugis Kurien, Microsoft, Midfin Systems, Sigcomm 2015</p>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[Hello World]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/hello-world.html"/>
        <published>2016-08-01T00:00:00+08:00</published>
        <updated>2016-08-04T10:10:13+08:00</updated>
        <id>http://vc2004.github.io/2016/hello-world.html</id>
        <category scheme="http://vc2004.github.io/tag/python/" term="python" label="python" />
        <category scheme="http://vc2004.github.io/tag/code/" term="code" label="code" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <p>Hello World. This is a DEMO post.</p>
]]>
        </content>
    </entry>
</feed>