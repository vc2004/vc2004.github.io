<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Thinking and Learning in Networking</title>
    <link href="http://vc2004.github.io/feed.xml" rel="self" />
    <link href="http://vc2004.github.io/" />
    <updated>2017-01-20T11:18:51+08:00</updated>
    <id>http://vc2004.github.io/</id>
    <entry>
        <title type="html"><![CDATA[Linux Networking Optimisation Guide Part III (Cont.)]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2017/linux-networking-optimisation-guide-iii.html"/>
        <published>2017-01-20T00:00:00+08:00</published>
        <updated>2017-01-20T11:18:51+08:00</updated>
        <id>http://vc2004.github.io/2017/linux-networking-optimisation-guide-iii.html</id>
        <category scheme="http://vc2004.github.io/tag/networking/" term="networking" label="networking" />
        <category scheme="http://vc2004.github.io/tag/linux/" term="Linux" label="Linux" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <h3 id="toc_0">MTU</h3>
<p>Change MTU to 9000 is going to help increase the throughput and efficiency on big packets. However all the routes/switches on the path should support jumbo-frame, e.g. over 9000.</p>

<pre><code>ip link set eth1 mtu 9000
ip link show
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
    link/ether 00:1e:c9:b4:86:0e brd ff:ff:ff:ff:ff:ff
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP mode DEFAULT group default qlen 1000
    link/ether 00:1e:c9:b4:86:10 brd ff:ff:ff:ff:ff:ff</code></pre>

<h3 id="toc_1">QLEN</h3>
<p>The length of the Queuing Discipline in the packet transfer process. Increasing this value may lead to bufferbloat with increasing latency.</p>
<p>Normally the queue length 1000 is enough for the 10G/40G network. However if the error on ip -s link or ifconfig -a eth0 is increasing, then try to increase the qlen.</p>

<pre><code>ip link show eth0
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000
    link/ether 00:22:19:5b:e2:f2 brd ff:ff:ff:ff:ff:ff

 ip -s link show eth0
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000
    link/ether 00:22:19:5b:e2:f2 brd ff:ff:ff:ff:ff:ff
    RX: bytes  packets  errors  dropped overrun mcast
    56272058545094 237996400274 10125   0       10125   790686739
    TX: bytes  packets  errors  dropped carrier collsns
    338460632172563 338969673742 0       0       0       0</code></pre>

<h3 id="toc_2">Power State</h3>
<p>CPU Power State other than C1 &amp; C0 should all be disabled, it should first be disabled in BIOS.</p>
<p>The processor.max_cstate=1 and intel_idle.max_cstate=0 could be added in the grub line to override BIOS setting.</p>

<pre><code>vi /etc/default/grub

GRUB_DEFAULT=0
GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian`
GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash processor.max_cstate=1 intel_idle.max_cstate=0&quot;
GRUB_CMDLINE_LINUX=&quot;clocksource=tsc ipv6.disable=1&quot;

 update-grub
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-3.16.0-4-amd64
Found initrd image: /boot/initrd.img-3.16.0-4-amd64
Found memtest86+ image: /boot/memtest86+.bin
Found memtest86+ multiboot image: /boot/memtest86+_multiboot.bin
done</code></pre>
<p>It could be confirmed by</p>

<pre><code>cat /sys/module/intel_idle/parameters/max_cstate
9</code></pre>

<h3 id="toc_3">Pause Frame</h3>
<p>Pause frame are sent out once the Tx and Rx are full to local switch port, if the switch support pause frame, the switch will pause sending the packets for orders of ms or less, which is pretty enough for the processing the Tx/Rx remaining buffer. Both switch and ethernet card on the server should support Pause frame.</p>

<pre><code>ethtool -a eth4
Pause parameters for eth4:
Autonegotiate:    off
RX:        on
TX:        on</code></pre>
<p>To turn on the Rx and Tx Pause Frame</p>

<pre><code>ethtool -A eth4 rx on</code></pre>

<h3 id="toc_4">TCP/UDP Parameter</h3>
<p>There are a lot of TCP parameters on linux could be optimised:</p>
<p>net.ipv4.tcp_timestamps = 1</p>
<p>Timestamp options could avoid wrapped sequence numbers, and improve window size and buffer calculation. If bandwidth is high and TCP sequence numbers is wrapped very quickly, do turn on the tcp time stamp option. But keep in mind it does increase the CPU usage.</p>

<pre><code>net.ipv4.tcp_sack = 0</code></pre>
<p>Selective ack could allow sender only transmit the lost bytes other than all the bytes. Turning on the sack may increase the CPU load. Unless there is a very high latency or high packet loss link, it is suggest to turn off the tcp_sack. There are still some controversy on turning off this option.</p>

<pre><code>net.ipv4.tcp_window_scaling = 1</code></pre>
<p>Originally the tcp window size is 8bit, it is not enough for the bandwidth nowadays. Turning on tcp window scaling to increase the window size, and both size should both support tcp window scale to finish a successful tcp negotiation. </p>

<pre><code>net.ipv4.tcp_rmem=4096 87380 16777216
net.ipv4.tcp_wmem=4096 65536 16777216
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.core.wmem_default = 2097152
net.core.rmem_default = 2097152
net.core.optmem_max = 524287</code></pre>
<p>wmem/rmem is the socket buffer size. TCP and UDP Rx and Tx buffer could be set to a large value to avoid packet errors. The command line netstat -us or -ts could be checked if there are some UDP or TCP related errors.</p>

<pre><code>net.core.somaxconn = 8192</code></pre>
<p>To avoid SYN Flood to stop new connection drop or SYN Cookie to send. The TCP listen back log should be increased to a larger number. It is the maximum number of unaccepted TCP connection the system could handle.</p>

<pre><code>net.ipv4.tcp_adv_win_scale = 1
net.ipv4.tcp_fin_timeout = 15</code></pre>
<p>To release resource more quickly by setting fin timeout to 15s. It is the timeout period of fin-wait-2 to closed. </p>

<h3 id="toc_5">MSI-X/NAPI</h3>
<p>Check if MSI-X is enabled, normally it is enabled by default. </p>

<pre><code># lspci -vvv | less
        Capabilities: [70] MSI-X: Enable+ Count=64 Masked-
                Vector table: BAR=4 offset=00000000
                PBA: BAR=4 offset=00002000</code></pre>
<p>Normally NAPI is enabled by default.</p>

<h3 id="toc_6">Module Parameters</h3>
<p>Some Module (bnx2x or ixgbe) parameters may need to adjusted to improve the performance.</p>
<p>For example, the num of queues could be adjusted by load the module with new parameter value on ‘num_queues’, do it on the console. Normally this value doesn’t need to be changed, the default value is number of the CPU.</p>
<p>modprobe -r bnx2x
modprobe bnx2x num_queues=2</p>

<pre><code>modinfo bnx2x
parm:           num_queues: Set number of queues (default is as a number of CPUs) (int)
parm:           disable_tpa: Disable the TPA (LRO) feature (int)
parm:           int_mode: Force interrupt mode other than MSI-X (1 INT#x; 2 MSI) (int)
parm:           dropless_fc: Pause on exhausted host ring (int)
parm:           mrrs: Force Max Read Req Size (0..3) (for debug) (int)
parm:           debug: Default debug msglevel (int)</code></pre>
<p>The value could be checked in </p>

<pre><code>cat /sys/module/bnx2x/parameters/int_mode
0</code></pre>
<p>For intel ixgbe, there are lots of parameter could be modified as well</p>

<pre><code>parm:           InterruptType:Change Interrupt Mode (0=Legacy, 1=MSI, 2=MSI-X), default IntMode (deprecated) (array of int)
parm:           IntMode:Change Interrupt Mode (0=Legacy, 1=MSI, 2=MSI-X), default 2 (array of int)
parm:           MQ:Disable or enable Multiple Queues, default 1 (array of int)
parm:           DCA:Disable or enable Direct Cache Access, 0=disabled, 1=descriptor only, 2=descriptor and data (array of int)
parm:           RSS:Number of Receive-Side Scaling Descriptor Queues, default 0=number of cpus (array of int)
parm:           VMDQ:Number of Virtual Machine Device Queues: 0/1 = disable, 2-16 enable (default=8) (array of int)
parm:           max_vfs:Number of Virtual Functions: 0 = disable (default), 1-63 = enable this many VFs (array of int)
parm:           VEPA:VEPA Bridge Mode: 0 = VEB (default), 1 = VEPA (array of int)
parm:           InterruptThrottleRate:Maximum interrupts per second, per vector, (0,1,956-488281), default 1 (array of int)
parm:           LLIPort:Low Latency Interrupt TCP Port (0-65535) (array of int)
parm:           LLIPush:Low Latency Interrupt on TCP Push flag (0,1) (array of int)
parm:           LLISize:Low Latency Interrupt on Packet Size (0-1500) (array of int)
parm:           LLIEType:Low Latency Interrupt Ethernet Protocol Type (array of int)
parm:           LLIVLANP:Low Latency Interrupt on VLAN priority threshold (array of int)
parm:           FdirPballoc:Flow Director packet buffer allocation level:
            1 = 8k hash filters or 2k perfect filters
            2 = 16k hash filters or 4k perfect filters
            3 = 32k hash filters or 8k perfect filters (array of int)
parm:           AtrSampleRate:Software ATR Tx packet sample rate (array of int)
parm:           FCoE:Disable or enable FCoE Offload, default 1 (array of int)
parm:           MDD:Malicious Driver Detection: (0,1), default 1 = on (array of int)
parm:           LRO:Large Receive Offload (0,1), default 0 = off (array of int)
parm:           allow_unsupported_sfp:Allow unsupported and untested SFP+ modules on 82599 based adapters, default 0 = Disable (array of int)
parm:           dmac_watchdog:DMA coalescing watchdog in microseconds (0,41-10000), default 0 = off (array of int)
parm:           vxlan_rx:VXLAN receive checksum offload (0,1), default 1 = Enable (array of int)</code></pre>

<h3 id="toc_7">Conntrack Parameters</h3>
<p>nf_conntrack_max and related hash size needs to optimise the conntrack based on the free memory. </p>

<pre><code># sysctl net.netfilter.nf_conntrack_max
net.netfilter.nf_conntrack_max = 1000000</code></pre>
<p>The hash size could be setup dynamically by changing the parameter </p>

<pre><code># echo 1000000 &gt; /sys/module/nf_conntrack/parameters/hashsize
# cat /sys/module/nf_conntrack/parameters/hashsize
1000448</code></pre>
<p>Normally the hash size is the 1/8 of the nf_conntrack_max. However in order to increase the efficiency, 1:1 could be set if you have enough memory.</p>
<p>In case of 1M nf_conntrack_max setting, the total memory used is </p>

<pre><code>total mem = conntrack_max * sizeof(struct ip_conntrack) + hash_size * sizeof(struct list_head)
          = 1M * 328B + 1M * 16B = 344MB</code></pre>
<p>There is a script for calculating the required memory, note the hash size and max is 1:8 in this case, it is not the optimised case.</p>

<pre><code>python conn_table_mem.py 1000
On this machine, each conntrack entry requires 328 bytes of kernel memory, and each hash table entry requires 16.

Therefore to consume a maximum of 1000 MiB of kernel memory:
 - conntrack_max should be set to 3177503
 - Using the kernel's default ratio, the nf_conntrack module's `hashsize' parameter should be set to 397188</code></pre>
<p>The timeout value could also be optimised </p>
<p>It is ok to set generic timeout to 30 ~ 120s, also the tcp_timeout_established should to modified to smaller value</p>

<pre><code>net.netfilter.nf_conntrack_generic_timeout = 120
net.netfilter.nf_conntrack_tcp_timeout_established = 86400</code></pre>
<p>Please note in the situation of long tcp sessions, for example, 5 days of single tcp connection of online gaming, it is better keep tcp_timeout_established valued to its original value.</p>

<h3 id="toc_8">Hardware Offload</h3>
<p>Hardware offload are ethernet card’s embedded function to offload some load from CPU. If the performance is extremely poor, tweak the GRO/TSO/LRO configuration. If UDP performance is extremely poor, try to turn off UFO. Likewise turn off the TSO, provided that the TCP performance is low. </p>

<pre><code>ethtool -k eth0
Features for eth0:
rx-checksumming: on
tx-checksumming: on
    tx-checksum-ipv4: on
    tx-checksum-ip-generic: off [fixed]
    tx-checksum-ipv6: on
    tx-checksum-fcoe-crc: off [fixed]
    tx-checksum-sctp: off [fixed]
scatter-gather: on
    tx-scatter-gather: on
    tx-scatter-gather-fraglist: off [fixed]
tcp-segmentation-offload: on
    tx-tcp-segmentation: on
    tx-tcp-ecn-segmentation: on
    tx-tcp6-segmentation: on
udp-fragmentation-offload: off [fixed]
generic-segmentation-offload: on
generic-receive-offload: on
large-receive-offload: off [fixed]</code></pre>
<p>As the result of the TSO/GSO, the data in the ring buffer will also greatly increase, consequently the latency would also raise. Tweaking these features to balance in CPU load/Throughput with Latencies.</p>

<h3 id="toc_9">Queueing Disciplines</h3>
<p>The default QDisc for linux is pfifo_fast, it is far from best queuing strategy because of the deep buffer in the single queue. As a result the latency will grow, coupled with bufferbloat effect. Moreover, the different traffic class may not get well prioritised by default pfifo_fast strategy. Multiple other choices could be selected, it seems now fq_codel is now the best choice. However the actually selection is quite depending on the actual traffic pattern.</p>

<h3 id="toc_10">Memory</h3>
<p>Each memory channel should have at least one memory DIMM(at least 4G) inserted to max performance.</p>

<pre><code>dmidecode -t memory | grep Locator
    Locator: PROC 1 DIMM 1
    Bank Locator: Not Specified
    Locator: PROC 1 DIMM 2
    Bank Locator: Not Specified
    Locator: PROC 1 DIMM 3
    Bank Locator: Not Specified
    Locator: PROC 1 DIMM 4
    Bank Locator: Not Specified
    Locator: PROC 1 DIMM 5
    Bank Locator: Not Specified
    Locator: PROC 1 DIMM 6
    Bank Locator: Not Specified
    Locator: PROC 1 DIMM 7
    Bank Locator: Not Specified
    Locator: PROC 1 DIMM 8
    Bank Locator: Not Specified
    Locator: PROC 2 DIMM 1
    Bank Locator: Not Specified
    Locator: PROC 2 DIMM 2
    Bank Locator: Not Specified
    Locator: PROC 2 DIMM 3
    Bank Locator: Not Specified
    Locator: PROC 2 DIMM 4
    Bank Locator: Not Specified
    Locator: PROC 2 DIMM 5
    Bank Locator: Not Specified
    Locator: PROC 2 DIMM 6
    Bank Locator: Not Specified
    Locator: PROC 2 DIMM 7
    Bank Locator: Not Specified
    Locator: PROC 2 DIMM 8
    Bank Locator: Not Specified</code></pre>

<h3 id="toc_11">PCI-E Slots</h3>
<p>Gen3 PCI-E slots have larger throughput than Gen2 PCI-E slots.</p>
<p>Make sure the PCI-slot with ethernet card inserted support speed with 20G or more, Gen2 slot typically does NOT support 20G or more bandwidth.</p>

<pre><code>lspci -s 04:00.0 -vvv | grep LnkSta
        LnkSta:    Speed 5GT/s, Width x8, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-
        LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete-, EqualizationPhase1-</code></pre>

<pre><code>dmidecode --type 9
# dmidecode 2.12
SMBIOS 2.8 present.

Handle 0x0900, DMI type 9, 17 bytes
System Slot Information
    Designation: PCIe Slot 1
    Type: x16 PCI Express 3
    Current Usage: Available
    Length: Long
    Characteristics:
        3.3 V is provided
        PME signal is supported

Handle 0x0901, DMI type 9, 17 bytes
System Slot Information
    Designation: PCIe Slot 2
    Type: x8 PCI Express 3 x16
    Current Usage: Available
    Length: Long
    Characteristics:
        3.3 V is provided
        PME signal is supported

Handle 0x0902, DMI type 9, 17 bytes
System Slot Information
    Designation: PCIe Slot 3
    Type: x8 PCI Express 3 x16
    Current Usage: In Use
    Length: Long
    Characteristics:
        3.3 V is provided
        PME signal is supported
    Bus Address: 0000:04:00.0</code></pre>
<p>5Gt/s * 8 Lanes = 40Gt/s * (8b/10b) = 32Gbps</p>
<p>So theoretically it could provide 32Gbps input/output</p>

<pre><code>May 26 11:37:30 kernel: [    2.899292] ixgbe 0000:04:00.0: PCI Express bandwidth of 32GT/s available
May 26 11:37:30 kernel: [    2.899294] ixgbe 0000:04:00.0: (Speed:5.0GT/s, Width: x8, Encoding Loss:20%)
May 26 11:37:30 kernel: [    2.899378] ixgbe 0000:04:00.0: MAC: 2, PHY: 15, SFP+: 5, PBA No: E66560-002</code></pre>
<p>For PCIe 2.0, the overhead is 2b/10b, for PCIe 3.0, the overhead is 2b/130b.</p>

<h3 id="toc_12">BIOS</h3>

<ul>
<li>Select max performance in Power Management Options</li>
<li>Disable CPU power state such as C6, C3, C1E or similar, leaving only C1 and C0</li>
<li>Turn on HT(Hyper Threading)</li>
<li>PCI configuration of ‘extended_tag’ has big impact on small packet performance of 40G ethernet interface. (also setpci could be used)</li>
</ul>

<h3 id="toc_13">Intel specific</h3>

<h4 id="toc_14">UDP Flow Hash</h4>
<p>Intel ethernet card support RSS, but on UDP packets. Ixgbe driver will send UDP packets with fragmentation bit set to CPU0 other than other CPUs. This default behaviour is set to avoid UDP packets out of order. This have performance degrade on UDP packets especially on VxLAN tunnel performance on 10G nic. </p>
<p>ethtool -N eth0 rx-flow-hash udp4 sdfn</p>

<h4 id="toc_15">Burst Length</h4>
<p>setpci modify the adapter’s configuration register to allow it to read up to 4k bytes at a time (Tx only). Use it with caution, it may lead some system to unstable state, restart to set it back or use value 22 to setpci back. </p>

<pre><code>lspci -nn | grep 82599
04:00.0 Ethernet controller [0200]: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection [8086:10fb] (rev 01)
04:00.1 Ethernet controller [0200]: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection [8086:10fb] (rev 01)</code></pre>

<pre><code>setpci -d 8086:1a48 e6.b=2e</code></pre>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[OVN and Geneve]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/ovn-and-geneve.html"/>
        <published>2016-12-15T00:00:00+08:00</published>
        <updated>2016-12-15T11:31:40+08:00</updated>
        <id>http://vc2004.github.io/2016/ovn-and-geneve.html</id>
        <category scheme="http://vc2004.github.io/tag/ovn/" term="ovn" label="ovn" />
        <category scheme="http://vc2004.github.io/tag/geneve/" term="geneve" label="geneve" />
        <category scheme="http://vc2004.github.io/tag/vxlan/" term="vxlan" label="vxlan" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <p>本文浅谈一下 OVN 和 Geneve，以及一点点应用和 code</p>

<h3 id="toc_0">Encapsulation in OVN</h3>
<p>OVN 支持三种隧道模式，Geneve，STT 和 VxLAN，但是其中 VxLAN 并不是什么情况下就能用的，Hypervisor 到 Hypervisor 之间的隧道模式只能走 Geneve 和 STT，到 GW 和 Vtep GW 的隧道才能用 VxLAN，这是为什么呢？</p>

<h3 id="toc_1">Why Geneve &amp; STT</h3>
<p>因为只有 STT 和 Geneve 支持携带大于 32bit 的 Metadata，VxLAN 并不支持这一特性。并且 STT 和 Geneve 支持使用随机的 UDP 和 TCP 源端口，这些包在 ECMP 里更容易被分布到不同的路径里，VxLAN 的固定端口很容易就打到一条路径上了。</p>
<p>STT 由于是 fake 出来的 TCP 包，网卡只要支持 TSO，就很容易达到高性能。VxLAN 现在一般网卡也都支持 Offloading了，但是就笔者经验，可能还有各种各样的问题。Geneve 比较新，也有新网卡支持了.</p>

<h3 id="toc_2">Geneve in OVN</h3>
<p>OVSDB 里的 Geneve tunnel 长这样</p>

<pre><code>Port &quot;ovn-711117-0&quot;
            Interface &quot;ovn-711117-0&quot;
                type: geneve
                options: {csum=&quot;true&quot;, key=flow, remote_ip=&quot;172.18.3.153&quot;}</code></pre>
<p>key=flow 含义是 VNI 由 flow 来决定。</p>
<p>拿一个 OVN 里的 Geneve 包来举例，</p>
<p><img src="https://github.com/vc2004/vc2004.github.io/raw/master/media/geneve.png" alt="Geneve in OVN"/></p>
<p>OVN 使用了 VNI 和 Options 来携带了 Metadata，其中</p>

<h4 id="toc_3">Logical Datapath as VNI</h4>
<p>VNI 使用了 Logical Datapath，也就是 0xb1, 这个和 southbound database 里 datapath_binding 表里的 tunnel key 一致</p>

<pre><code>_uuid               : 8fc46e14-1c0e-4129-a123-a69bf093c04e
external_ids        : {logical-switch=&quot;182eaadd-2cc3-4ff3-9bef-3793bb2463ec&quot;, name=&quot;neutron-f3dc2e30-f3e8-472b-abf8-ed455fc928f4&quot;}
tunnel_key          : 177</code></pre>

<h4 id="toc_4">Options</h4>
<p>Options 里携带了一个 OVN 的 TLV，其中 Option Data 为 0001002，其中第一个0是保留位。后面的 001 和 002 是 Logical Inpurt Port 和 Logical Output Port，和 southbound database 里的 port_biding 表里的 tunnel key 一致。</p>

<pre><code>_uuid              : e40c929d-1997-4fac-bad3-867996eebd03
chassis            : 869e09ab-d47e-4f18-8562-e28692dc0b39
datapath           : 8fc46e14-1c0e-4129-a123-a69bf093c04e
logical_port       : &quot;dedf0130-50eb-480d-9030-13b826093c4f&quot;
mac                : [&quot;fa:16:3e:ae:9a:b6 192.168.7.13&quot;]
options            : {}
parent_port        : []
tag                : []
tunnel_key         : 1
type               : &quot;&quot;

_uuid              : b410ed4b-de0f-4d66-9815-1ea56b0a833c
chassis            : be5e84f9-3d01-431b-bdfa-208411c102c9
datapath           : 8fc46e14-1c0e-4129-a123-a69bf093c04e
logical_port       : &quot;a3347aa1-a8fb-4e30-820c-04c7e1459dd3&quot;
mac                : [&quot;fa:16:3e:01:73:be 192.168.7.14&quot;]
options            : {}
parent_port        : []
tag                : []
tunnel_key         : 2
type               : &quot;&quot;</code></pre>

<h3 id="toc_5">Show Me The Code</h3>
<p>在 ovn/controller/physical.h 中，定义 Class 为0x0102 和 type 0x80，可以看到和上图一致。</p>

<pre><code>#define OVN_GENEVE_CLASS 0x0102  /* Assigned Geneve class for OVN. */
#define OVN_GENEVE_TYPE 0x80     /* Critical option. */
#define OVN_GENEVE_LEN 4</code></pre>
<p>在 ovn/controller/physical.c 中，可以看到 ovn-controller 在 encapsulation 的时候，如果是 Geneve，会把 datapath的 tunnel key 放到 MFF_TUN_ID 里，outport 和 inport 放到 mff_ovn_geneve 里。</p>

<pre><code>static void
put_encapsulation(enum mf_field_id mff_ovn_geneve,
                  const struct chassis_tunnel *tun,
                  const struct sbrec_datapath_binding *datapath,
                  uint16_t outport, struct ofpbuf *ofpacts)
{
    if (tun-&gt;type == GENEVE) {
        put_load(datapath-&gt;tunnel_key, MFF_TUN_ID, 0, 24, ofpacts);
        put_load(outport, mff_ovn_geneve, 0, 32, ofpacts);
        put_move(MFF_LOG_INPORT, 0, mff_ovn_geneve, 16, 15, ofpacts);
    } else if (tun-&gt;type == STT) {
        put_load(datapath-&gt;tunnel_key | (outport &lt;&lt; 24), MFF_TUN_ID, 0, 64,
                 ofpacts);
        put_move(MFF_LOG_INPORT, 0, MFF_TUN_ID, 40, 15, ofpacts);
    } else if (tun-&gt;type == VXLAN) {
        put_load(datapath-&gt;tunnel_key, MFF_TUN_ID, 0, 24, ofpacts);
    } else {
        OVS_NOT_REACHED();
    }
}</code></pre>
<p>在头文件定义里，可以看到 MFF_TUN_ID 就是 VNI</p>

<pre><code>/* &quot;tun_id&quot; (aka &quot;tunnel_id&quot;).
     *
     * The &quot;key&quot; or &quot;tunnel ID&quot; or &quot;VNI&quot; in a packet received via a keyed
     * tunnel.  For protocols in which the key is shorter than 64 bits, the key
     * is stored in the low bits and the high bits are zeroed.  For non-keyed
     * tunnels and packets not received via a tunnel, the value is 0.
     *
     * Type: be64.
     * Maskable: bitwise.
     * Formatting: hexadecimal.
     * Prerequisites: none.
     * Access: read/write.
     * NXM: NXM_NX_TUN_ID(16) since v1.1.
     * OXM: OXM_OF_TUNNEL_ID(38) since OF1.3 and v1.10.
     * Prefix lookup member: tunnel.tun_id.
     */

    MFF_TUN_ID,</code></pre>

<h3 id="toc_6">Reference</h3>

<ul>
<li>OVN Architecture: <a href="http://openvswitch.org/support/dist-docs/ovn-architecture.7.html">openvswitch.org/support/dist-docs/ovn-architecture.7.html</a></li>
<li>OVN Repo: <a href="https://github.com/openvswitch/ovs/tree/master/ovn">github.com/openvswitch/ovs/tree/master/ovn</a></li>
</ul>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[Linux Networking Optimisation Guide Part II (Cont.)]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/linux-networking-optimisation-guide-ii.html"/>
        <published>2016-12-13T00:00:00+08:00</published>
        <updated>2016-12-13T10:15:37+08:00</updated>
        <id>http://vc2004.github.io/2016/linux-networking-optimisation-guide-ii.html</id>
        <category scheme="http://vc2004.github.io/tag/networking/" term="networking" label="networking" />
        <category scheme="http://vc2004.github.io/tag/linux/" term="Linux" label="Linux" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <h3 id="toc_0">Taskset/Isolcpus</h3>
<p>It is better to pin the process on specific CPUs and make that CPU isolated</p>
<p>In the following example, CPU 1-15, 17-31 are isolated, leaving CPU 0 and CPU 32 processing the other system and user task. Those CPU could be used to pin KVM/QEMU or netperf session.</p>

<pre><code>#vi /etc/default/grub

GRUB_DEFAULT=0
GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian`
GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash default_hugepagesz=1G hugepagesz=1G hugepages=16 hugepagesz=2M hugepages=2048 intel_iommu=off isolcpus=1-15,17-31&quot;
GRUB_CMDLINE_LINUX=&quot;clocksource=tsc ipv6.disable=1&quot;

#update-grub

Generating grub configuration file ...
Found linux image: /boot/vmlinuz-3.16.0-4-amd64
Found initrd image: /boot/initrd.img-3.16.0-4-amd64
Found memtest86+ image: /boot/memtest86+.bin
Found memtest86+ multiboot image: /boot/memtest86+_multiboot.bin
done</code></pre>

<h3 id="toc_1">RPS</h3>
<p>For those ethernet card which DO NOT support RSS or multiple queue, it is better configure RPS. If RSS is already configured, then RPS may lead some performance drop.</p>
<p>In addition, if a particular traffic receiving on the server is dominant (GRE/UDP/specific TCP), RPS may have better performance than RSS.</p>

<pre><code>#/etc/init.d/set_rps.sh status

/sys/class/net/eth0/queues/rx-0/rps_cpus 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,000000ff
/sys/class/net/eth0/queues/rx-0/rps_flow_cnt 4096
/sys/class/net/eth1/queues/rx-0/rps_cpus 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,000000ff
/sys/class/net/eth1/queues/rx-0/rps_flow_cnt 4096
/proc/sys/net/core/rps_sock_flow_entries 8192</code></pre>
<p>In the scenario that high load of GRE or UDP packets are not RSS to the different CPU, try use RPS instead. </p>

<h3 id="toc_2">NIC ring buffer</h3>

<ul>
<li>Increasing ring buffer may lead to higher latency in certain circumstance (mainly on transmit end), where as the throughput is increased as well.</li>
<li>If packet are receiving too fast, small ring buffer may need to packet loss.</li>
</ul>
<p>Tune to the maximum of pre-set on Rx setting. For example, configure to 2040 on eth0</p>

<pre><code>#ethtool -g eth0

Ring parameters for eth0:
Pre-set maximums:
RX:        2040
RX Mini:    0
RX Jumbo:    8160
TX:        255
Current hardware settings:
RX:        2040
RX Mini:    0
RX Jumbo:    0
TX:        255</code></pre>

<pre><code>ethtool -G eth0 rx 2040</code></pre>

<h3 id="toc_3">SoftIRQ</h3>
<p>To check the SoftIRQ</p>

<pre><code>watch -n1 grep RX /proc/softirqs
watch -n1 grep TX /proc/softirqs</code></pre>
<p>In the /proc/net/softnet_stat</p>

<pre><code>#cat /proc/net/softnet_stat

a1f2d52e 0003bb1a 008bf81d 00000000 00000000 00000000 00000000 00000000 00000000 d2711d13 00000000
aba687fe 00038122 008ac060 00000000 00000000 00000000 00000000 00000000 00000000 cb49f8b4 00000000
ce145ec6 0003d512 008ae0bf 00000000 00000000 00000000 00000000 00000000 00000000 f08f0303 00000000
15dc9267 0003c306 0087799a 00000000 00000000 00000000 00000000 00000000 00000000 b0960b17 00000000
0660ac7b 0003bf5d 00872565 00000000 00000000 00000000 00000000 00000000 00000000 e437ec3d 00000000
d2578b45 0003e436 00835c20 00000000 00000000 00000000 00000000 00000000 00000000 e9dd448e 00000000
136320b0 0003732e 0087ca4a 00000000 00000000 00000000 00000000 00000000 00000000 b92b37df 00000000
e11fee84 0003b335 008cb2bb 00000000 00000000 00000000 00000000 00000000 00000000 ee76d89a 00000000</code></pre>
<p>In kernel version 3.16, according the related code, the  </p>

<pre><code>146 static int softnet_seq_show(struct seq_file *seq, void *v)
147 {
148        struct softnet_data *sd = v;
149        unsigned int flow_limit_count = 0;
150
151 #ifdef CONFIG_NET_FLOW_LIMIT
152        struct sd_flow_limit *fl;
153
154        rcu_read_lock();
155        fl = rcu_dereference(sd-&gt;flow_limit);
156        if (fl)
157                flow_limit_count = fl-&gt;count;
158        rcu_read_unlock();
159 #endif
160
161        seq_printf(seq,
162                    &quot;%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n&quot;,
163                    sd-&gt;processed, sd-&gt;dropped, sd-&gt;time_squeeze, 0,
164                    0, 0, 0, 0, /* was fastroute */
165                    sd-&gt;cpu_collision, sd-&gt;received_rps, flow_limit_count);
166        return 0;
167 }</code></pre>
<p>So the column names is processed, dropped, time_squeeze, null, null, null, null, null, cpu_collision, received_rps, flow_limit_count. And each row represent the different CPU.</p>
<p>if the third column is increasing, increase the netdev_budget. The polling routine has a budget which determines the CPU time the code is allowed. This is required to prevent SoftIRQs from monopolizing the CPU. The 3rd column is the number of times ksoftirqd ran out of netdev_budget or CPU time when there was still work to be done (time_squeeze). The more messages the CPU get from the buffer (600), the more time the SoftIRQ spent on CPU to process more messages. It avoid buffer flows. </p>

<pre><code>sysctl -w net.core.netdev_budget=600</code></pre>
<p>The netdev_budget meaning:</p>

<pre><code>netdev_budget
-------------

Maximum number of packets taken from all interfaces in one polling cycle (NAPI
poll). In one polling cycle interfaces which are registered to polling are
probed in a round-robin manner.</code></pre>

<h3 id="toc_4">Adapter Queue</h3>
<p>If the second column of softnet_stat is increasing, the frame is dropping at the queue. To avoid dropping the packets, the netdev_max_backlog should be increased. This option increase the maximum queue length (Within Kernel, before processing by the IP Stack, and after the NIC receive the packets) to avoid overflow.</p>

<pre><code>netdev_max_backlog
------------------

Maximum number  of  packets,  queued  on  the  INPUT  side, when the interface
receives packets faster than kernel can process them.</code></pre>
<p>It is the reverse side of qlen, the start point should be set to 300000</p>

<pre><code>sysctl -w net.core.netdev_max_backlog=300000
net.core.netdev_max_backlog = 300000</code></pre>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[Linux Networking Optimisation Guide Part I]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/linux-networking-optimisation-guide.html"/>
        <published>2016-09-21T00:00:00+08:00</published>
        <updated>2016-09-21T11:05:01+08:00</updated>
        <id>http://vc2004.github.io/2016/linux-networking-optimisation-guide.html</id>
        <category scheme="http://vc2004.github.io/tag/networking/" term="networking" label="networking" />
        <category scheme="http://vc2004.github.io/tag/linux/" term="Linux" label="Linux" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <h3 id="toc_0">Foreword:</h3>

<ul>
<li>Plain Linux installation is NOT optimised for the best networking performance</li>
<li>Almost all the optimisations have side affect. It is better to test before using it.</li>
</ul>

<h3 id="toc_1">Interrupt Affinity</h3>
<p>CPU Affinity is the most important and most effective optimisation, also it is the entry level optimisation.</p>
<p>Turn off irqbalance if any, note it may cause performance issue on other Hardware/IO devices.</p>

<pre><code>/etc/init.d/irqbalance stop</code></pre>
<p>The Rx queue could be checked by -l and modified by -L</p>

<pre><code># ethtool -l eth4
Channel parameters for eth4:
Pre-set maximums:
RX:        0
TX:        0
Other:        1
Combined:    63
Current hardware settings:
RX:        0
TX:        0
Other:        1
Combined:    32</code></pre>
<p>Check the interrupt number related to the eth0</p>

<pre><code># egrep &quot;CPU0|eth0&quot; /proc/interrupts
            CPU0       CPU1       CPU2       CPU3       CPU4       CPU5       CPU6       CPU7       CPU8       CPU9       CPU10      CPU11      CPU12      CPU13      CPU14      CPU15      CPU16      CPU17      CPU18      CPU19      CPU20      CPU21      CPU22      CPU23      CPU24      CPU25      CPU26      CPU27      CPU28      CPU29      CPU30      CPU31
 148:     347358          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0
 150:         18    1152920          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-0
 151:         27          0      61465          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-1
 152:         10          0          0      32140          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-2
 153:         37          0          0          0     113157          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-3
 154:         10          0          0          0          0      89395          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-4
 155:         11          0          0          0          0          0      75379          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-5
 156:          8          0          0          0          0          0          0     123974          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-6
 157:          5          0          0          0          0          0          0          0     277624          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-7</code></pre>
<p>Echo the cpu bit mask to related interrupt number</p>

<pre><code>echo 00000001 &gt; /proc/irq/148/smp_affinity</code></pre>
<p>Tips: MAC native calculator is very good at calculating cpu bit mask.</p>

<h3 id="toc_2">Interrupt Coalescence</h3>
<p>Interrupt Coalescence (IC) is the number of usec waited or frames gathered to issue a hardware interrupt. A small value or big value both has side affects. If latency is preferred over throughput, eg. the realtime streaming traffic, a small value or be disabled would be benefit. Otherwise for large throughput, a larger value should be selected.</p>

<pre><code>ethtool -c eth0
Coalesce parameters for eth0:
Adaptive RX: off  TX: off
stats-block-usecs: 999936
sample-interval: 0
pkt-rate-low: 0
pkt-rate-high: 0

rx-usecs: 18
rx-frames: 12
rx-usecs-irq: 18
rx-frames-irq: 2

tx-usecs: 80
tx-frames: 20
tx-usecs-irq: 18
tx-frames-irq: 2

rx-usecs-low: 0
rx-frame-low: 0
tx-usecs-low: 0
tx-frame-low: 0

rx-usecs-high: 0
rx-frame-high: 0
tx-usecs-high: 0
tx-frame-high: 0</code></pre>
<p>Some cards support adaptive changing the parameters, just turn on the adapter rx and tx.
<code>
ethtool -C eth0 adaptive-rx on adaptive-tx on
</code></p>

<h3 id="toc_3">NUMA</h3>
<p>The network performance might increase if the NUMA node is close to the PCIe slot with ethernet card attached. But it is very tricky the performance might drop with the Tx/Rx application is on the different NUMA node or on the same logical core. So do tweaking a lots to get the best performance.</p>
<p>It is known that:</p>

<ul>
<li>Two child process on different numa node with cause L3 miss, so the performance will drop.</li>
<li>Two child process on the same logical core(HT) will cause performance drop.</li>
<li>Performance will be optimised if two child process on same numa node with memory also on that node.</li>
</ul>
<p>For example, if two child process on same core 0 [0,16], the performance will drop. The same will happen on cpu0 and cpu8.</p>
<p>So it is important to decide which cpu to pin Tx/Rx netsurf application, otherwise you might get a drop</p>

<pre><code># python cpu_layout.py --status
============================================================
Core and Socket Information (as reported by '/proc/cpuinfo')
============================================================

cores =  [0, 1, 2, 3, 4, 5, 6, 7]
sockets =  [0, 1]

       Socket 0        Socket 1
       --------        --------
Core 0 [0, 16]         [8, 24]

Core 1 [1, 17]         [9, 25]

Core 2 [2, 18]         [10, 26]

Core 3 [3, 19]         [11, 27]

Core 4 [4, 20]         [12, 28]

Core 5 [5, 21]         [13, 29]

Core 6 [6, 22]         [14, 30]

Core 7 [7, 23]         [15, 31]</code></pre>
<p>For best performance on NUMA, check which NUMA node the PCIe are connected to</p>

<pre><code># lspci -tv
 \-[0000:00]-+-00.0  Intel Corporation Haswell-E DMI2
             +-01.0-[02]--
             +-01.1-[05]--
             +-02.0-[06]--+-00.0  Broadcom Corporation BCM57840 NetXtreme II 10/20-Gigabit Ethernet
             |            \-00.1  Broadcom Corporation BCM57840 NetXtreme II 10/20-Gigabit Ethernet

# cat /sys/devices/pci0000\:00/0000\:00\:02.0/numa_node
0

# cat /sys/devices/pci0000\:00/0000\:00\:02.0/local_cpus
00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00ff00ff</code></pre>
<p>0xFF00FF is CPU0-7 &amp; 16-23, so it is better to set the affinity on CPU0-7 or CPU16-23.</p>
<p>Also, if two or more ports from different NIC are used, make sure they are connected to the same CPU socket.</p>

<h3 id="toc_4">CPU Frequency</h3>
<p>To maximise the CPU frequency to handle the network loads, it could be set through OS.</p>

<pre><code>cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
powersave

echo performance &gt; /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor</code></pre>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[测试一下中文，顺便推荐几门上过的 Data Science 公开课]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/chinese-testing.html"/>
        <published>2016-09-01T00:00:00+08:00</published>
        <updated>2016-09-22T12:03:42+08:00</updated>
        <id>http://vc2004.github.io/2016/chinese-testing.html</id>
        <category scheme="http://vc2004.github.io/tag/r/" term="R" label="R" />
        <category scheme="http://vc2004.github.io/tag/statistic/" term="Statistic" label="Statistic" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <h3 id="toc_0">测试一下中文，顺便推荐几门上过的 Data Science 公开课</h3>
<p>This post is testing if language other than english is working on this site. Also some cool free open course for data science are recommended here.</p>

<h4 id="toc_1">Statistical Learning by Stanford</h4>
<p><a href="https://statlearning.class.stanford.edu">statlearning.class.stanford.edu</a></p>
<p>两个教授 Trevor Hastie 和 Rob Tibshirani 都是大牛, Prof Rob在 Lasso 和 Bootstrap Sampling 都有挺重要的贡献。</p>

<h4 id="toc_2">Machine Learning on Coursera</h4>
<p><a href="https://www.coursera.org/learn/machine-learning">www.coursera.org/learn/machine-learning</a></p>
<p>这个就不用多介绍了。。Ng 的牛课，Coursera 代表课之一，深入浅出。。我还想学一遍。。</p>

<h4 id="toc_3">Data Science Specialisation on Coursera</h4>
<p><a href="https://www.coursera.org/specializations/jhu-data-science">www.coursera.org/specializations/jhu-data-science</a></p>
<p>JHU 开的课，这个课总体来说还可以，但是有点小问题是有些课过于简单，有些课又过于复杂，不如前面两门。</p>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[Networking Fundamental and Recent Advance Reading List]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/recent-networking-advance-readlist.html"/>
        <published>2016-08-03T00:00:00+08:00</published>
        <updated>2016-12-15T10:12:27+08:00</updated>
        <id>http://vc2004.github.io/2016/recent-networking-advance-readlist.html</id>
        <category scheme="http://vc2004.github.io/tag/networking/" term="networking" label="networking" />
        <category scheme="http://vc2004.github.io/tag/sdn/" term="sdn" label="sdn" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <p>Complied by Liang Dong</p>

<h3 id="toc_0">Update</h3>

<ul>
<li>2016-12-13 Add several paper about BBR congestion control and P4 Dataplane Programming</li>
<li>2016-09-20 Add two thesis about load balancing service in Cloud Networking</li>
</ul>

<h3 id="toc_1">Foreword</h3>
<p>This is a small reading list about recent advance on networking. It also cover some basic fundamental knowledges of TCP/IP.</p>

<h3 id="toc_2">Data Center Networking:</h3>
<p><strong>A Scalable, Commodity Data Center Network Architecture</strong>, Mohammad Al-Fares, Alexander Loukissas, Amin Vahdat, University of California, San Diego, Sigcomm 2008</p>
<p><strong>Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network</strong>, Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, Anand Kanagala, Jeff Provost, Jason Simmons, Eiichi Tanda, Jim Wanderer, Urs Hölzle, Stephen Stuart, and Amin Vahdat. Sigcomm 2016.</p>
<p><strong>Inside the Social Network’s (Datacenter) Network, Arjun Roy</strong>, Hongyi Zeng, Jasmeet Bagga, George Porter, and Alex C. Snoeren, Sigcomm 2015</p>
<p><strong>Introducing data center fabric, the next-generation Facebook data centre network</strong>, Alexey Andreyev, Facebook</p>

<h3 id="toc_3">Software Defined Networking:</h3>
<p><strong>Openflow: Enable Innovation in Campus Networks</strong>, Nick McKeown, Tom Anderson, Hari Balakrishnan, Guru Parulkar, Larry Peterson, Jennifer Rexford, Scott Shenker, Jonathan Turner, CCR 2008</p>
<p><strong>The Future of Networking, and the Past of Protocols</strong> - Scott Shenker</p>
<p><strong>The Design and Implementation of Open vSwitch</strong>, Ben Pfaff, Justin Pettit, Teemu Koponen, Ethan Jackson, Andy Zhou, Jarno Rajahalme, Jesse Gross, Alex Wang, Joe Stringer, and Pravin Shelar, VMware, Inc.; Keith Amidon, Awake Networks; Martín Casado, VMware, Inc, NSDI 2015</p>
<p><strong>B4: Experience with a Globally-Deployed Software Defined WAN</strong>, Sushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong, Leon Poutievski, Arjun Singh, Subbaiah Venkata, Jim Wanderer, Junlan Zhou, Min Zhu, Jonathan Zolla, Urs Hölzle, Stephen Stuart and Amin Vahdat, Sigcomm 2013</p>

<h3 id="toc_4">Dataplane Programming</h3>
<p><strong>P4: Programming Protocol-Independent Packet Processors</strong>, Pat Bosshart, Dan Daly, Glen Gibb, Martin Izzard, Nick McKeown, Jennifer Rexford, Cole Schlesinger, Dan Talayco, Amin Vahdat, George Varghese, David Walker</p>

<h3 id="toc_5">Network Virtualisation &amp; Cloud Networking:</h3>
<p><strong>VL2: A Scalable and Flexible Data Centre Network</strong>, Albert Greenberg, Srikanth Kandula,  David A. Maltz , James R. Hamilton, Changhoon Kim, Parveen Patel , Navendu Jain, Parantap Lahiri, Sudipta Sengupta, Microsoft Research, Sigcomm 2009</p>
<p><strong>Network Virtualization in Multi-tenant Datacenters</strong>, Teemu Koponen, Keith Amidon, Peter Balland, Martín Casado, Anupam Chanda, Bryan Fulton, Igor Ganichev, Jesse Gross, Natasha Gude, Paul Ingram, Ethan Jackson, Andrew Lambeth, Romain Lenglet, Shih-Hao Li, Amar Padmanabhan, Justin Pettit, Ben Pfaff, and Rajiv Ramanathan, VMware; Scott Shenker, International Computer Science Institute and the University of California, Berkeley; Alan Shieh, Jeremy Stribling, Pankaj Thakkar, Dan Wendlandt, Alexander Yip, and Ronghua Zhang, VMware, NSDI 2014</p>
<p><strong>Maglev: A Fast and Reliable Software Network Load Balancer</strong>, Daniel E. Eisenbud, Cheng Yi, Carlo Contavalli, Cody Smith, Roman Kononov, Eric Mann-Hielscher, Ardas Cilingiroglu, and Bin Cheyney, Google Inc.; Wentao Shang, University of California, Los Angeles; Jinnah Dylan Hosein, SpaceX</p>
<p><strong>Ananta: Cloud Scale Load Balancing</strong>, Parveen Patel, Deepak Bansal, Lihua Yuan, Ashwin Murthy, Albert Greenberg, David A. Maltz, Randy Kern, Hemant Kumar, Marios Zikos, Hongyu Wu, Changhoon Kim, Naveen Karri, Microsoft.</p>

<h3 id="toc_6">Transport Protocols:</h3>
<p><strong>RFC 5681, TCP Congestion Control</strong>, IETF</p>
<p><strong>Data Center TCP (DCTCP)</strong>, Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, Murari Sridharan, Sigcomm 2010</p>
<p><strong>Understanding TCP Incast Throughput Collapse in Datacenter Networks</strong>, Yanpei Chen, Rean Griffith, Junda Liu, Randy H. Katz, Anthony D. Joseph, Sigcomm 2009</p>
<p><strong>Bufferbloat: Dark Buffers in the Internet, Networks without effective AQM may again be vulnerable to congestion collapse</strong>, Jim Gettys, Bell Labs, Alcatel-Lucent; and Kathleen Nichols, Pollere Inc, ACMQueue Volume 9, issue 11</p>
<p><strong>BBR: Congestion-Based Congestion Control, Measuring bottleneck bandwidth and round-trip propagation time</strong>, Neal Cardwell, Yuchung Cheng, C. Stephen Gunn, Soheil Hassas Yeganeh, Van Jacobson, ACMQueue Volume 14, issue 5</p>

<h3 id="toc_7">Network Measurement:</h3>
<p><strong>Pingmesh: A Large-Scale System for Data CenterNetwork Latency Measurement and Analysis</strong>, Chuanxiong Guo, Lihua Yuan, Dong Xiang, Yingnong Dang, Ray Huang, Dave Maltz, Zhaoyi Liu, Vin Wang, Bin Pang, Hua Chen, Zhi-Wei Lin, Varugis Kurien, Microsoft, Midfin Systems, Sigcomm 2015</p>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[Hello World]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/hello-world.html"/>
        <published>2016-08-01T00:00:00+08:00</published>
        <updated>2016-08-04T10:10:13+08:00</updated>
        <id>http://vc2004.github.io/2016/hello-world.html</id>
        <category scheme="http://vc2004.github.io/tag/python/" term="python" label="python" />
        <category scheme="http://vc2004.github.io/tag/code/" term="code" label="code" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <p>Hello World. This is a DEMO post.</p>
]]>
        </content>
    </entry>
</feed>