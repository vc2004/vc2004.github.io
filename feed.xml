<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Thinking and Learning in Networking</title>
    <link href="http://vc2004.github.io/feed.xml" rel="self" />
    <link href="http://vc2004.github.io/" />
    <updated>2016-12-15T11:22:46+08:00</updated>
    <id>http://vc2004.github.io/</id>
    <entry>
        <title type="html"><![CDATA[OVN and Geneve]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/ovn-and-geneve.html"/>
        <published>2016-12-15T00:00:00+08:00</published>
        <updated>2016-12-15T11:22:46+08:00</updated>
        <id>http://vc2004.github.io/2016/ovn-and-geneve.html</id>
        <category scheme="http://vc2004.github.io/tag/ovn/" term="ovn" label="ovn" />
        <category scheme="http://vc2004.github.io/tag/geneve/" term="geneve" label="geneve" />
        <category scheme="http://vc2004.github.io/tag/vxlan/" term="vxlan" label="vxlan" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <p>本文浅谈一下 OVN 和 Geneve，以及一点点应用和 code</p>

<h3 id="toc_0">Encapsulation in OVN</h3>
<p>OVN 支持三种隧道模式，Geneve，STT 和 VxLAN，但是其中 VxLAN 并不是什么情况下就能用的，Hypervisor 到 Hypervisor 之间的隧道模式只能走 Geneve 和 STT，到 GW 和 Vtep GW 的隧道才能用 VxLAN，这是为什么呢？</p>

<h3 id="toc_1">Why Geneve &amp; STT</h3>
<p>因为只有 STT 和 Geneve 支持携带大于 32bit 的 Metadata，VxLAN 并不支持这一特性。并且 STT 和 Geneve 支持使用随机的 UDP 和 TCP 源端口，这些包在 ECMP 里更容易被分布到不同的路径里，VxLAN 的固定端口很容易就打到一条路径上了。</p>
<p>STT 由于是 fake 出来的 TCP 包，网卡只要支持 TSO，就很容易达到高性能。VxLAN 现在一般网卡也都支持 Offloading了，但是就笔者经验，可能还有各种各样的问题。Geneve 比较新，也有新网卡支持了.</p>

<h3 id="toc_2">Geneve in OVN</h3>
<p>OVSDB 里的 Geneve tunnel 长这样</p>

<pre><code>Port &quot;ovn-711117-0&quot;
            Interface &quot;ovn-711117-0&quot;
                type: geneve
                options: {csum=&quot;true&quot;, key=flow, remote_ip=&quot;172.18.3.153&quot;}</code></pre>
<p>key=flow 含义是 VNI 由 flow 来决定。</p>
<p>拿一个 OVN 里的 Geneve 包来举例，</p>
<p><img src="https://github.com/vc2004/vc2004.github.io/raw/master/media/geneve.png" alt="Geneve in OVN"/></p>
<p>OVN 使用了 VNI 和 Options 来携带了 Metadata，其中</p>

<h4 id="toc_3">Logical Datapath as VNI</h4>
<p>VNI 使用了 Logical Datapath，也就是 0xb1, 这个和 southbound database 里 datapath_binding 表里的 tunnel key 一致</p>

<pre><code>_uuid               : 8fc46e14-1c0e-4129-a123-a69bf093c04e
external_ids        : {logical-switch=&quot;182eaadd-2cc3-4ff3-9bef-3793bb2463ec&quot;, name=&quot;neutron-f3dc2e30-f3e8-472b-abf8-ed455fc928f4&quot;}
tunnel_key          : 177</code></pre>

<h4 id="toc_4">Options</h4>
<p>Options 里携带了一个 OVN 的 TLV，其中 Option Data 为 0001002，其中第一个0是保留位。后面的 001 和 002 是 Logical Inpurt Port 和 Logical Output Port，和 southbound database 里的 port_biding 表里的 tunnel key 一致。</p>

<pre><code>_uuid              : e40c929d-1997-4fac-bad3-867996eebd03
chassis            : 869e09ab-d47e-4f18-8562-e28692dc0b39
datapath           : 8fc46e14-1c0e-4129-a123-a69bf093c04e
logical_port       : &quot;dedf0130-50eb-480d-9030-13b826093c4f&quot;
mac                : [&quot;fa:16:3e:ae:9a:b6 192.168.7.13&quot;]
options            : {}
parent_port        : []
tag                : []
tunnel_key         : 1
type               : &quot;&quot;

_uuid              : b410ed4b-de0f-4d66-9815-1ea56b0a833c
chassis            : be5e84f9-3d01-431b-bdfa-208411c102c9
datapath           : 8fc46e14-1c0e-4129-a123-a69bf093c04e
logical_port       : &quot;a3347aa1-a8fb-4e30-820c-04c7e1459dd3&quot;
mac                : [&quot;fa:16:3e:01:73:be 192.168.7.14&quot;]
options            : {}
parent_port        : []
tag                : []
tunnel_key         : 2
type               : &quot;&quot;</code></pre>

<h3 id="toc_5">Show Me The Code</h3>
<p>在 ovn/controller/physical.h 中，定义 Class 为0x0102 和 type 0x80，可以看到和上图一致。</p>

<pre><code>#define OVN_GENEVE_CLASS 0x0102  /* Assigned Geneve class for OVN. */
#define OVN_GENEVE_TYPE 0x80     /* Critical option. */
#define OVN_GENEVE_LEN 4</code></pre>
<p>在 ovn/controller/physical.c 中，可以看到 ovn-controller 在 encapsulation 的时候，如果是 Geneve，会把 datapath的 tunnel key 放到 MFF_TUN_ID 里，outport 和 inport 放到 mff_ovn_geneve 里。</p>

<pre><code>static void
put_encapsulation(enum mf_field_id mff_ovn_geneve,
                  const struct chassis_tunnel *tun,
                  const struct sbrec_datapath_binding *datapath,
                  uint16_t outport, struct ofpbuf *ofpacts)
{
    if (tun-&gt;type == GENEVE) {
        put_load(datapath-&gt;tunnel_key, MFF_TUN_ID, 0, 24, ofpacts);
        put_load(outport, mff_ovn_geneve, 0, 32, ofpacts);
        put_move(MFF_LOG_INPORT, 0, mff_ovn_geneve, 16, 15, ofpacts);
    } else if (tun-&gt;type == STT) {
        put_load(datapath-&gt;tunnel_key | (outport &lt;&lt; 24), MFF_TUN_ID, 0, 64,
                 ofpacts);
        put_move(MFF_LOG_INPORT, 0, MFF_TUN_ID, 40, 15, ofpacts);
    } else if (tun-&gt;type == VXLAN) {
        put_load(datapath-&gt;tunnel_key, MFF_TUN_ID, 0, 24, ofpacts);
    } else {
        OVS_NOT_REACHED();
    }
}</code></pre>
<p>在头文件定义里，可以看到 MFF_TUN_ID 就是 VNI</p>

<pre><code>/* &quot;tun_id&quot; (aka &quot;tunnel_id&quot;).
     *
     * The &quot;key&quot; or &quot;tunnel ID&quot; or &quot;VNI&quot; in a packet received via a keyed
     * tunnel.  For protocols in which the key is shorter than 64 bits, the key
     * is stored in the low bits and the high bits are zeroed.  For non-keyed
     * tunnels and packets not received via a tunnel, the value is 0.
     *
     * Type: be64.
     * Maskable: bitwise.
     * Formatting: hexadecimal.
     * Prerequisites: none.
     * Access: read/write.
     * NXM: NXM_NX_TUN_ID(16) since v1.1.
     * OXM: OXM_OF_TUNNEL_ID(38) since OF1.3 and v1.10.
     * Prefix lookup member: tunnel.tun_id.
     */

    MFF_TUN_ID,</code></pre>

<h3 id="toc_6">Reference</h3>
<p>OVN Architecture: <a href="http://openvswitch.org/support/dist-docs/ovn-architecture.7.html">openvswitch.org/support/dist-docs/ovn-architecture.7.html</a>
OVN Repo: <a href="https://github.com/openvswitch/ovs/tree/master/ovn">github.com/openvswitch/ovs/tree/master/ovn</a></p>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[Linux Networking Optimisation Guide Part II (Cont.)]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/linux-networking-optimisation-guide-ii.html"/>
        <published>2016-12-13T00:00:00+08:00</published>
        <updated>2016-12-13T10:15:37+08:00</updated>
        <id>http://vc2004.github.io/2016/linux-networking-optimisation-guide-ii.html</id>
        <category scheme="http://vc2004.github.io/tag/networking/" term="networking" label="networking" />
        <category scheme="http://vc2004.github.io/tag/linux/" term="Linux" label="Linux" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <h3 id="toc_0">Taskset/Isolcpus</h3>
<p>It is better to pin the process on specific CPUs and make that CPU isolated</p>
<p>In the following example, CPU 1-15, 17-31 are isolated, leaving CPU 0 and CPU 32 processing the other system and user task. Those CPU could be used to pin KVM/QEMU or netperf session.</p>

<pre><code>#vi /etc/default/grub

GRUB_DEFAULT=0
GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian`
GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash default_hugepagesz=1G hugepagesz=1G hugepages=16 hugepagesz=2M hugepages=2048 intel_iommu=off isolcpus=1-15,17-31&quot;
GRUB_CMDLINE_LINUX=&quot;clocksource=tsc ipv6.disable=1&quot;

#update-grub

Generating grub configuration file ...
Found linux image: /boot/vmlinuz-3.16.0-4-amd64
Found initrd image: /boot/initrd.img-3.16.0-4-amd64
Found memtest86+ image: /boot/memtest86+.bin
Found memtest86+ multiboot image: /boot/memtest86+_multiboot.bin
done</code></pre>

<h3 id="toc_1">RPS</h3>
<p>For those ethernet card which DO NOT support RSS or multiple queue, it is better configure RPS. If RSS is already configured, then RPS may lead some performance drop.</p>
<p>In addition, if a particular traffic receiving on the server is dominant (GRE/UDP/specific TCP), RPS may have better performance than RSS.</p>

<pre><code>#/etc/init.d/set_rps.sh status

/sys/class/net/eth0/queues/rx-0/rps_cpus 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,000000ff
/sys/class/net/eth0/queues/rx-0/rps_flow_cnt 4096
/sys/class/net/eth1/queues/rx-0/rps_cpus 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,000000ff
/sys/class/net/eth1/queues/rx-0/rps_flow_cnt 4096
/proc/sys/net/core/rps_sock_flow_entries 8192</code></pre>
<p>In the scenario that high load of GRE or UDP packets are not RSS to the different CPU, try use RPS instead. </p>

<h3 id="toc_2">NIC ring buffer</h3>

<ul>
<li>Increasing ring buffer may lead to higher latency in certain circumstance (mainly on transmit end), where as the throughput is increased as well.</li>
<li>If packet are receiving too fast, small ring buffer may need to packet loss.</li>
</ul>
<p>Tune to the maximum of pre-set on Rx setting. For example, configure to 2040 on eth0</p>

<pre><code>#ethtool -g eth0

Ring parameters for eth0:
Pre-set maximums:
RX:        2040
RX Mini:    0
RX Jumbo:    8160
TX:        255
Current hardware settings:
RX:        2040
RX Mini:    0
RX Jumbo:    0
TX:        255</code></pre>

<pre><code>ethtool -G eth0 rx 2040</code></pre>

<h3 id="toc_3">SoftIRQ</h3>
<p>To check the SoftIRQ</p>

<pre><code>watch -n1 grep RX /proc/softirqs
watch -n1 grep TX /proc/softirqs</code></pre>
<p>In the /proc/net/softnet_stat</p>

<pre><code>#cat /proc/net/softnet_stat

a1f2d52e 0003bb1a 008bf81d 00000000 00000000 00000000 00000000 00000000 00000000 d2711d13 00000000
aba687fe 00038122 008ac060 00000000 00000000 00000000 00000000 00000000 00000000 cb49f8b4 00000000
ce145ec6 0003d512 008ae0bf 00000000 00000000 00000000 00000000 00000000 00000000 f08f0303 00000000
15dc9267 0003c306 0087799a 00000000 00000000 00000000 00000000 00000000 00000000 b0960b17 00000000
0660ac7b 0003bf5d 00872565 00000000 00000000 00000000 00000000 00000000 00000000 e437ec3d 00000000
d2578b45 0003e436 00835c20 00000000 00000000 00000000 00000000 00000000 00000000 e9dd448e 00000000
136320b0 0003732e 0087ca4a 00000000 00000000 00000000 00000000 00000000 00000000 b92b37df 00000000
e11fee84 0003b335 008cb2bb 00000000 00000000 00000000 00000000 00000000 00000000 ee76d89a 00000000</code></pre>
<p>In kernel version 3.16, according the related code, the  </p>

<pre><code>146 static int softnet_seq_show(struct seq_file *seq, void *v)
147 {
148        struct softnet_data *sd = v;
149        unsigned int flow_limit_count = 0;
150
151 #ifdef CONFIG_NET_FLOW_LIMIT
152        struct sd_flow_limit *fl;
153
154        rcu_read_lock();
155        fl = rcu_dereference(sd-&gt;flow_limit);
156        if (fl)
157                flow_limit_count = fl-&gt;count;
158        rcu_read_unlock();
159 #endif
160
161        seq_printf(seq,
162                    &quot;%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n&quot;,
163                    sd-&gt;processed, sd-&gt;dropped, sd-&gt;time_squeeze, 0,
164                    0, 0, 0, 0, /* was fastroute */
165                    sd-&gt;cpu_collision, sd-&gt;received_rps, flow_limit_count);
166        return 0;
167 }</code></pre>
<p>So the column names is processed, dropped, time_squeeze, null, null, null, null, null, cpu_collision, received_rps, flow_limit_count. And each row represent the different CPU.</p>
<p>if the third column is increasing, increase the netdev_budget. The polling routine has a budget which determines the CPU time the code is allowed. This is required to prevent SoftIRQs from monopolizing the CPU. The 3rd column is the number of times ksoftirqd ran out of netdev_budget or CPU time when there was still work to be done (time_squeeze). The more messages the CPU get from the buffer (600), the more time the SoftIRQ spent on CPU to process more messages. It avoid buffer flows. </p>

<pre><code>sysctl -w net.core.netdev_budget=600</code></pre>
<p>The netdev_budget meaning:</p>

<pre><code>netdev_budget
-------------

Maximum number of packets taken from all interfaces in one polling cycle (NAPI
poll). In one polling cycle interfaces which are registered to polling are
probed in a round-robin manner.</code></pre>

<h3 id="toc_4">Adapter Queue</h3>
<p>If the second column of softnet_stat is increasing, the frame is dropping at the queue. To avoid dropping the packets, the netdev_max_backlog should be increased. This option increase the maximum queue length (Within Kernel, before processing by the IP Stack, and after the NIC receive the packets) to avoid overflow.</p>

<pre><code>netdev_max_backlog
------------------

Maximum number  of  packets,  queued  on  the  INPUT  side, when the interface
receives packets faster than kernel can process them.</code></pre>
<p>It is the reverse side of qlen, the start point should be set to 300000</p>

<pre><code>sysctl -w net.core.netdev_max_backlog=300000
net.core.netdev_max_backlog = 300000</code></pre>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[Linux Networking Optimisation Guide Part I]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/linux-networking-optimisation-guide.html"/>
        <published>2016-09-21T00:00:00+08:00</published>
        <updated>2016-09-21T11:05:01+08:00</updated>
        <id>http://vc2004.github.io/2016/linux-networking-optimisation-guide.html</id>
        <category scheme="http://vc2004.github.io/tag/networking/" term="networking" label="networking" />
        <category scheme="http://vc2004.github.io/tag/linux/" term="Linux" label="Linux" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <h3 id="toc_0">Foreword:</h3>

<ul>
<li>Plain Linux installation is NOT optimised for the best networking performance</li>
<li>Almost all the optimisations have side affect. It is better to test before using it.</li>
</ul>

<h3 id="toc_1">Interrupt Affinity</h3>
<p>CPU Affinity is the most important and most effective optimisation, also it is the entry level optimisation.</p>
<p>Turn off irqbalance if any, note it may cause performance issue on other Hardware/IO devices.</p>

<pre><code>/etc/init.d/irqbalance stop</code></pre>
<p>The Rx queue could be checked by -l and modified by -L</p>

<pre><code># ethtool -l eth4
Channel parameters for eth4:
Pre-set maximums:
RX:        0
TX:        0
Other:        1
Combined:    63
Current hardware settings:
RX:        0
TX:        0
Other:        1
Combined:    32</code></pre>
<p>Check the interrupt number related to the eth0</p>

<pre><code># egrep &quot;CPU0|eth0&quot; /proc/interrupts
            CPU0       CPU1       CPU2       CPU3       CPU4       CPU5       CPU6       CPU7       CPU8       CPU9       CPU10      CPU11      CPU12      CPU13      CPU14      CPU15      CPU16      CPU17      CPU18      CPU19      CPU20      CPU21      CPU22      CPU23      CPU24      CPU25      CPU26      CPU27      CPU28      CPU29      CPU30      CPU31
 148:     347358          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0
 150:         18    1152920          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-0
 151:         27          0      61465          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-1
 152:         10          0          0      32140          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-2
 153:         37          0          0          0     113157          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-3
 154:         10          0          0          0          0      89395          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-4
 155:         11          0          0          0          0          0      75379          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-5
 156:          8          0          0          0          0          0          0     123974          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-6
 157:          5          0          0          0          0          0          0          0     277624          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0          0  IR-PCI-MSI-edge      eth0-fp-7</code></pre>
<p>Echo the cpu bit mask to related interrupt number</p>

<pre><code>echo 00000001 &gt; /proc/irq/148/smp_affinity</code></pre>
<p>Tips: MAC native calculator is very good at calculating cpu bit mask.</p>

<h3 id="toc_2">Interrupt Coalescence</h3>
<p>Interrupt Coalescence (IC) is the number of usec waited or frames gathered to issue a hardware interrupt. A small value or big value both has side affects. If latency is preferred over throughput, eg. the realtime streaming traffic, a small value or be disabled would be benefit. Otherwise for large throughput, a larger value should be selected.</p>

<pre><code>ethtool -c eth0
Coalesce parameters for eth0:
Adaptive RX: off  TX: off
stats-block-usecs: 999936
sample-interval: 0
pkt-rate-low: 0
pkt-rate-high: 0

rx-usecs: 18
rx-frames: 12
rx-usecs-irq: 18
rx-frames-irq: 2

tx-usecs: 80
tx-frames: 20
tx-usecs-irq: 18
tx-frames-irq: 2

rx-usecs-low: 0
rx-frame-low: 0
tx-usecs-low: 0
tx-frame-low: 0

rx-usecs-high: 0
rx-frame-high: 0
tx-usecs-high: 0
tx-frame-high: 0</code></pre>
<p>Some cards support adaptive changing the parameters, just turn on the adapter rx and tx.
<code>
ethtool -C eth0 adaptive-rx on adaptive-tx on
</code></p>

<h3 id="toc_3">NUMA</h3>
<p>The network performance might increase if the NUMA node is close to the PCIe slot with ethernet card attached. But it is very tricky the performance might drop with the Tx/Rx application is on the different NUMA node or on the same logical core. So do tweaking a lots to get the best performance.</p>
<p>It is known that:</p>

<ul>
<li>Two child process on different numa node with cause L3 miss, so the performance will drop.</li>
<li>Two child process on the same logical core(HT) will cause performance drop.</li>
<li>Performance will be optimised if two child process on same numa node with memory also on that node.</li>
</ul>
<p>For example, if two child process on same core 0 [0,16], the performance will drop. The same will happen on cpu0 and cpu8.</p>
<p>So it is important to decide which cpu to pin Tx/Rx netsurf application, otherwise you might get a drop</p>

<pre><code># python cpu_layout.py --status
============================================================
Core and Socket Information (as reported by '/proc/cpuinfo')
============================================================

cores =  [0, 1, 2, 3, 4, 5, 6, 7]
sockets =  [0, 1]

       Socket 0        Socket 1
       --------        --------
Core 0 [0, 16]         [8, 24]

Core 1 [1, 17]         [9, 25]

Core 2 [2, 18]         [10, 26]

Core 3 [3, 19]         [11, 27]

Core 4 [4, 20]         [12, 28]

Core 5 [5, 21]         [13, 29]

Core 6 [6, 22]         [14, 30]

Core 7 [7, 23]         [15, 31]</code></pre>
<p>For best performance on NUMA, check which NUMA node the PCIe are connected to</p>

<pre><code># lspci -tv
 \-[0000:00]-+-00.0  Intel Corporation Haswell-E DMI2
             +-01.0-[02]--
             +-01.1-[05]--
             +-02.0-[06]--+-00.0  Broadcom Corporation BCM57840 NetXtreme II 10/20-Gigabit Ethernet
             |            \-00.1  Broadcom Corporation BCM57840 NetXtreme II 10/20-Gigabit Ethernet

# cat /sys/devices/pci0000\:00/0000\:00\:02.0/numa_node
0

# cat /sys/devices/pci0000\:00/0000\:00\:02.0/local_cpus
00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00ff00ff</code></pre>
<p>0xFF00FF is CPU0-7 &amp; 16-23, so it is better to set the affinity on CPU0-7 or CPU16-23.</p>
<p>Also, if two or more ports from different NIC are used, make sure they are connected to the same CPU socket.</p>

<h3 id="toc_4">CPU Frequency</h3>
<p>To maximise the CPU frequency to handle the network loads, it could be set through OS.</p>

<pre><code>cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
powersave

echo performance &gt; /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor</code></pre>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[测试一下中文，顺便推荐几门上过的 Data Science 公开课]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/chinese-testing.html"/>
        <published>2016-09-01T00:00:00+08:00</published>
        <updated>2016-09-22T12:03:42+08:00</updated>
        <id>http://vc2004.github.io/2016/chinese-testing.html</id>
        <category scheme="http://vc2004.github.io/tag/r/" term="R" label="R" />
        <category scheme="http://vc2004.github.io/tag/statistic/" term="Statistic" label="Statistic" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <h3 id="toc_0">测试一下中文，顺便推荐几门上过的 Data Science 公开课</h3>
<p>This post is testing if language other than english is working on this site. Also some cool free open course for data science are recommended here.</p>

<h4 id="toc_1">Statistical Learning by Stanford</h4>
<p><a href="https://statlearning.class.stanford.edu">statlearning.class.stanford.edu</a></p>
<p>两个教授 Trevor Hastie 和 Rob Tibshirani 都是大牛, Prof Rob在 Lasso 和 Bootstrap Sampling 都有挺重要的贡献。</p>

<h4 id="toc_2">Machine Learning on Coursera</h4>
<p><a href="https://www.coursera.org/learn/machine-learning">www.coursera.org/learn/machine-learning</a></p>
<p>这个就不用多介绍了。。Ng 的牛课，Coursera 代表课之一，深入浅出。。我还想学一遍。。</p>

<h4 id="toc_3">Data Science Specialisation on Coursera</h4>
<p><a href="https://www.coursera.org/specializations/jhu-data-science">www.coursera.org/specializations/jhu-data-science</a></p>
<p>JHU 开的课，这个课总体来说还可以，但是有点小问题是有些课过于简单，有些课又过于复杂，不如前面两门。</p>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[Networking Fundamental and Recent Advance Reading List]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/recent-networking-advance-readlist.html"/>
        <published>2016-08-03T00:00:00+08:00</published>
        <updated>2016-12-15T10:12:27+08:00</updated>
        <id>http://vc2004.github.io/2016/recent-networking-advance-readlist.html</id>
        <category scheme="http://vc2004.github.io/tag/networking/" term="networking" label="networking" />
        <category scheme="http://vc2004.github.io/tag/sdn/" term="sdn" label="sdn" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <p>Complied by Liang Dong</p>

<h3 id="toc_0">Update</h3>

<ul>
<li>2016-12-13 Add several paper about BBR congestion control and P4 Dataplane Programming</li>
<li>2016-09-20 Add two thesis about load balancing service in Cloud Networking</li>
</ul>

<h3 id="toc_1">Foreword</h3>
<p>This is a small reading list about recent advance on networking. It also cover some basic fundamental knowledges of TCP/IP.</p>

<h3 id="toc_2">Data Center Networking:</h3>
<p><strong>A Scalable, Commodity Data Center Network Architecture</strong>, Mohammad Al-Fares, Alexander Loukissas, Amin Vahdat, University of California, San Diego, Sigcomm 2008</p>
<p><strong>Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network</strong>, Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, Anand Kanagala, Jeff Provost, Jason Simmons, Eiichi Tanda, Jim Wanderer, Urs Hölzle, Stephen Stuart, and Amin Vahdat. Sigcomm 2016.</p>
<p><strong>Inside the Social Network’s (Datacenter) Network, Arjun Roy</strong>, Hongyi Zeng, Jasmeet Bagga, George Porter, and Alex C. Snoeren, Sigcomm 2015</p>
<p><strong>Introducing data center fabric, the next-generation Facebook data centre network</strong>, Alexey Andreyev, Facebook</p>

<h3 id="toc_3">Software Defined Networking:</h3>
<p><strong>Openflow: Enable Innovation in Campus Networks</strong>, Nick McKeown, Tom Anderson, Hari Balakrishnan, Guru Parulkar, Larry Peterson, Jennifer Rexford, Scott Shenker, Jonathan Turner, CCR 2008</p>
<p><strong>The Future of Networking, and the Past of Protocols</strong> - Scott Shenker</p>
<p><strong>The Design and Implementation of Open vSwitch</strong>, Ben Pfaff, Justin Pettit, Teemu Koponen, Ethan Jackson, Andy Zhou, Jarno Rajahalme, Jesse Gross, Alex Wang, Joe Stringer, and Pravin Shelar, VMware, Inc.; Keith Amidon, Awake Networks; Martín Casado, VMware, Inc, NSDI 2015</p>
<p><strong>B4: Experience with a Globally-Deployed Software Defined WAN</strong>, Sushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong, Leon Poutievski, Arjun Singh, Subbaiah Venkata, Jim Wanderer, Junlan Zhou, Min Zhu, Jonathan Zolla, Urs Hölzle, Stephen Stuart and Amin Vahdat, Sigcomm 2013</p>

<h3 id="toc_4">Dataplane Programming</h3>
<p><strong>P4: Programming Protocol-Independent Packet Processors</strong>, Pat Bosshart, Dan Daly, Glen Gibb, Martin Izzard, Nick McKeown, Jennifer Rexford, Cole Schlesinger, Dan Talayco, Amin Vahdat, George Varghese, David Walker</p>

<h3 id="toc_5">Network Virtualisation &amp; Cloud Networking:</h3>
<p><strong>VL2: A Scalable and Flexible Data Centre Network</strong>, Albert Greenberg, Srikanth Kandula,  David A. Maltz , James R. Hamilton, Changhoon Kim, Parveen Patel , Navendu Jain, Parantap Lahiri, Sudipta Sengupta, Microsoft Research, Sigcomm 2009</p>
<p><strong>Network Virtualization in Multi-tenant Datacenters</strong>, Teemu Koponen, Keith Amidon, Peter Balland, Martín Casado, Anupam Chanda, Bryan Fulton, Igor Ganichev, Jesse Gross, Natasha Gude, Paul Ingram, Ethan Jackson, Andrew Lambeth, Romain Lenglet, Shih-Hao Li, Amar Padmanabhan, Justin Pettit, Ben Pfaff, and Rajiv Ramanathan, VMware; Scott Shenker, International Computer Science Institute and the University of California, Berkeley; Alan Shieh, Jeremy Stribling, Pankaj Thakkar, Dan Wendlandt, Alexander Yip, and Ronghua Zhang, VMware, NSDI 2014</p>
<p><strong>Maglev: A Fast and Reliable Software Network Load Balancer</strong>, Daniel E. Eisenbud, Cheng Yi, Carlo Contavalli, Cody Smith, Roman Kononov, Eric Mann-Hielscher, Ardas Cilingiroglu, and Bin Cheyney, Google Inc.; Wentao Shang, University of California, Los Angeles; Jinnah Dylan Hosein, SpaceX</p>
<p><strong>Ananta: Cloud Scale Load Balancing</strong>, Parveen Patel, Deepak Bansal, Lihua Yuan, Ashwin Murthy, Albert Greenberg, David A. Maltz, Randy Kern, Hemant Kumar, Marios Zikos, Hongyu Wu, Changhoon Kim, Naveen Karri, Microsoft.</p>

<h3 id="toc_6">Transport Protocols:</h3>
<p><strong>RFC 5681, TCP Congestion Control</strong>, IETF</p>
<p><strong>Data Center TCP (DCTCP)</strong>, Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, Murari Sridharan, Sigcomm 2010</p>
<p><strong>Understanding TCP Incast Throughput Collapse in Datacenter Networks</strong>, Yanpei Chen, Rean Griffith, Junda Liu, Randy H. Katz, Anthony D. Joseph, Sigcomm 2009</p>
<p><strong>Bufferbloat: Dark Buffers in the Internet, Networks without effective AQM may again be vulnerable to congestion collapse</strong>, Jim Gettys, Bell Labs, Alcatel-Lucent; and Kathleen Nichols, Pollere Inc, ACMQueue Volume 9, issue 11</p>
<p><strong>BBR: Congestion-Based Congestion Control, Measuring bottleneck bandwidth and round-trip propagation time</strong>, Neal Cardwell, Yuchung Cheng, C. Stephen Gunn, Soheil Hassas Yeganeh, Van Jacobson, ACMQueue Volume 14, issue 5</p>

<h3 id="toc_7">Network Measurement:</h3>
<p><strong>Pingmesh: A Large-Scale System for Data CenterNetwork Latency Measurement and Analysis</strong>, Chuanxiong Guo, Lihua Yuan, Dong Xiang, Yingnong Dang, Ray Huang, Dave Maltz, Zhaoyi Liu, Vin Wang, Bin Pang, Hua Chen, Zhi-Wei Lin, Varugis Kurien, Microsoft, Midfin Systems, Sigcomm 2015</p>
]]>
        </content>
    </entry><entry>
        <title type="html"><![CDATA[Hello World]]></title>
        <author><name>Liang Dong</name></author>
        <link href="http://vc2004.github.io/2016/hello-world.html"/>
        <published>2016-08-01T00:00:00+08:00</published>
        <updated>2016-08-04T10:10:13+08:00</updated>
        <id>http://vc2004.github.io/2016/hello-world.html</id>
        <category scheme="http://vc2004.github.io/tag/python/" term="python" label="python" />
        <category scheme="http://vc2004.github.io/tag/code/" term="code" label="code" />
        <content type="html" xml:base="http://vc2004.github.io/" xml:lang="en">
            <![CDATA[ <p>Hello World. This is a DEMO post.</p>
]]>
        </content>
    </entry>
</feed>